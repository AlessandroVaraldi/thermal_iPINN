#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_ipinn_synthetic_odeforward.py

Training and comparison of MLP / TCN / LSTM on a synthetic thermal dataset
generated by make_synthetic_thermal_dataset.py, using a model where the
thermal ODE is explicitly integrated in the forward pass.

New version:
  - we estimate R_cond and C_th
  - R_conv per scenario is FIXED and read from true_params.json
  - one scenario is held out from training and used as a "serious" final test
    on its full sequence length
  - for each model we save training curves for:
      * train/val losses
      * R_cond
      * C_th
      * u-regularization term

Physical model (used in the forward pass):

  C_th * dT/dt = P(t) + u(t)
                 - (T - T_bplate)/R_cond
                 - (T - T_amb)/R_conv_scenario

where:
  - C_th is learnable (initialized as C_th_true * Cth_init_scale)
  - R_cond is learnable (global, shared across scenarios)
  - R_conv_scenario is FIXED (one per scenario, from JSON)
  - u(t) is a small correction term produced by the NN (ideally small)

The neural network (MLP/TCN/LSTM) outputs u_k(t) given the input sequence
[P(t), T_bplate(t), T_amb(t)]. Then T(t) is obtained by integrating the ODE
forward in time (explicit Euler discretization).

Loss:
  L = MSE(T_pred, T_case_meas) + lambda_u * mean(u^2)

Assumptions on the synthetic dataset:
  - Directory with:
      scenario_000.csv, scenario_001.csv, ...
      true_params.json

  - Each CSV has columns:
      time_s          [s]
      P_W             [W]
      Tamb_C          [°C]
      T_bplate_C      [°C]
      T_case_true_C   [°C]
      T_case_meas_C   [°C]

  - true_params.json contains:
      C_th_true
      R_cond_true
      scenarios: [
        { "scenario_id": j, "R_conv_true": ..., "csv_file": "scenario_XXX.csv", ...}
      ]

Example usage:

  python train_ipinn_synthetic_odeforward.py \
      --data-dir synthetic_thermal \
      --outdir synth_results_ode \
      --arch all \
      --window 512 \
      --epochs 80 \
      --lambda-u 1e-3 \
      --device cpu \
      --test-scen-id 5
"""

import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# Dataset for synthetic CSV files
# ---------------------------------------------------------------------
def robust_dt(seconds: np.ndarray) -> float:
    """Robust estimate of dt (median between p10 and p90 of positive deltas)."""
    ds = np.diff(seconds.astype(float))
    ds = ds[np.isfinite(ds) & (ds > 0)]
    if ds.size == 0:
        return 0.1
    p10, p90 = np.percentile(ds, [10, 90])
    core = ds[(ds >= p10) & (ds <= p90)]
    return float(np.median(core if core.size else ds))


class SyntheticThermalWindowDataset(Dataset):
    """
    Windowed dataset for synthetic thermal CSV files.

    Each item returns:
      - inputs: (T,3) -> [P_W, T_bplate_C, Tamb_C] normalized
      - T_case_meas: (T,) (measured target)
      - T_case_true: (T,) (ground truth)
      - mask: (T,) 1 where data is valid, 0 where padding
      - P_W: (T,)
      - T_bplate: (T,)
      - Tamb: (T,)
      - dt: scalar [s]
      - scenario_id: internal scenario index (0..N_train-1)
    """

    def __init__(
        self,
        csv_paths: List[str],
        window_size: int = 512,
        stride: Optional[int] = None,
        preload: bool = True,
    ):
        super().__init__()
        self.paths = sorted(csv_paths)
        self.n_scenarios = len(self.paths)
        self.window = int(window_size)
        self.stride = int(stride) if stride is not None else max(1, self.window // 2)

        # map path -> internal scenario_id
        self.scenario_id_by_path: Dict[str, int] = {
            p: i for i, p in enumerate(self.paths)
        }

        # optional dataframe cache
        self._cache: Dict[str, pd.DataFrame] = {}

        # list of windows
        self._windows: List[Dict] = []
        self._build_windows(preload=preload)

        # normalization statistics for input [P, T_bplate, Tamb]
        self.input_mean = np.zeros(3, dtype=float)
        self.input_std = np.ones(3, dtype=float)

    def _read_csv(self, path: str) -> pd.DataFrame:
        if path in self._cache:
            return self._cache[path]
        df = pd.read_csv(path)
        self._cache[path] = df
        return df

    def _build_windows(self, preload: bool = True):
        for p in self.paths:
            df = self._read_csv(p)
            if "time_s" not in df.columns:
                raise RuntimeError(f"{p}: 'time_s' column not found.")
            t = df["time_s"].to_numpy(dtype=float)
            dt_est = robust_dt(t)
            n = len(df)
            if n <= 1:
                continue

            scen_id = self.scenario_id_by_path[p]

            if self.window <= 0 or self.window >= n:
                # single full-length window
                self._windows.append(
                    {"path": p, "start": 0, "dt": dt_est, "scenario_id": scen_id}
                )
            else:
                for start in range(0, max(1, n - self.window + 1), self.stride):
                    self._windows.append(
                        {"path": p, "start": start, "dt": dt_est, "scenario_id": scen_id}
                    )

        if not self._windows:
            raise RuntimeError("No windows generated. Check window_size and CSV files.")

    # ---- compute mean/std for inputs ----
    def compute_normalization(self, n_samples_max: int = 5000, rng_seed: int = 7):
        rng = np.random.default_rng(rng_seed)
        if len(self._windows) == 0:
            return

        idx = np.arange(len(self._windows))
        rng.shuffle(idx)
        idx = idx[: min(len(idx), n_samples_max)]

        P_all, Tb_all, Tamb_all = [], [], []

        for i in idx:
            arr = self._extract_window(self._windows[i])
            P_all.append(arr["P_W"])
            Tb_all.append(arr["T_bplate"])
            Tamb_all.append(arr["Tamb"])

        def concat_valid(lst):
            if not lst:
                return np.array([])
            cat = np.concatenate(lst)
            return cat[np.isfinite(cat)]

        P_cat = concat_valid(P_all)
        Tb_cat = concat_valid(Tb_all)
        Tamb_cat = concat_valid(Tamb_all)

        self.input_mean[0] = float(np.mean(P_cat)) if P_cat.size else 0.0
        self.input_std[0]  = float(np.std(P_cat)) if P_cat.size else 1.0

        self.input_mean[1] = float(np.mean(Tb_cat)) if Tb_cat.size else 25.0
        self.input_std[1]  = float(np.std(Tb_cat)) if Tb_cat.size else 1.0

        self.input_mean[2] = float(np.mean(Tamb_cat)) if Tamb_cat.size else 25.0
        self.input_std[2]  = float(np.std(Tamb_cat)) if Tamb_cat.size else 1.0

        self.input_std = np.where(self.input_std < 1e-6, 1.0, self.input_std)

        print("[Dataset] Normalization:")
        print("  input_mean:", self.input_mean)
        print("  input_std :", self.input_std)

    # ---- raw window extraction ----
    def _extract_window(self, item: Dict) -> Dict:
        path = item["path"]
        start = int(item["start"])
        df = self._read_csv(path)

        # required columns
        for col in ["time_s", "P_W", "Tamb_C", "T_bplate_C", "T_case_true_C", "T_case_meas_C"]:
            if col not in df.columns:
                raise RuntimeError(f"{path}: column '{col}' not found.")

        t = df["time_s"].to_numpy(dtype=float)
        P = df["P_W"].to_numpy(dtype=float)
        Tamb = df["Tamb_C"].to_numpy(dtype=float)
        Tb = df["T_bplate_C"].to_numpy(dtype=float)
        Ttrue = df["T_case_true_C"].to_numpy(dtype=float)
        Tmeas = df["T_case_meas_C"].to_numpy(dtype=float)

        n = len(t)
        end = min(start + self.window, n)
        sl = slice(start, end)

        t_win = t[sl]
        P_win = P[sl]
        Tamb_win = Tamb[sl]
        Tb_win = Tb[sl]
        Ttrue_win = Ttrue[sl]
        Tmeas_win = Tmeas[sl]

        dt_local = robust_dt(t_win) if len(t_win) > 1 else float(item["dt"])

        return {
            "time_s": t_win,
            "P_W": P_win,
            "Tamb": Tamb_win,
            "T_bplate": Tb_win,
            "T_case_true": Ttrue_win,
            "T_case_meas": Tmeas_win,
            "dt": dt_local,
            "scenario_id": item["scenario_id"],
        }

    # ---- Dataset API ----
    def __len__(self) -> int:
        return len(self._windows)

    def __getitem__(self, idx: int) -> Dict:
        return self._extract_window(self._windows[idx])

    # ---- collate_fn for DataLoader ----
    def collate_fn(self, batch: List[Dict]) -> Dict:
        B = len(batch)
        T = self.window

        inputs_raw = np.zeros((B, T, 3), dtype=float)  # [P, T_bplate, Tamb]
        T_case_meas = np.zeros((B, T), dtype=float)
        T_case_true = np.zeros((B, T), dtype=float)
        mask = np.zeros((B, T), dtype=float)
        P_W = np.zeros((B, T), dtype=float)
        T_bplate = np.zeros((B, T), dtype=float)
        Tamb = np.zeros((B, T), dtype=float)
        dt_arr = np.zeros((B,), dtype=float)
        scen_ids = np.zeros((B,), dtype=np.int64)

        for i, item in enumerate(batch):
            P_win = item["P_W"]
            Tb_win = item["T_bplate"]
            Tamb_win = item["Tamb"]
            Ttrue_win = item["T_case_true"]
            Tmeas_win = item["T_case_meas"]

            L = len(P_win)
            inputs_raw[i, :L, 0] = P_win
            inputs_raw[i, :L, 1] = Tb_win
            inputs_raw[i, :L, 2] = Tamb_win

            T_case_meas[i, :L] = Tmeas_win
            T_case_true[i, :L] = Ttrue_win
            mask[i, :L] = 1.0

            P_W[i, :L] = P_win
            T_bplate[i, :L] = Tb_win
            Tamb[i, :L] = Tamb_win

            dt_arr[i] = item["dt"]
            scen_ids[i] = item["scenario_id"]

        # normalization
        x = inputs_raw.copy()
        for ch in range(3):
            nan_mask = ~np.isfinite(x[:, :, ch])
            x[:, :, ch][nan_mask] = self.input_mean[ch]

        mean = self.input_mean.reshape(1, 1, 3)
        std = self.input_std.reshape(1, 1, 3)
        inputs_norm = (x - mean) / std

        batch_out = {
            "inputs":      torch.tensor(inputs_norm, dtype=torch.float32),       # (B,T,3)
            "T_case_meas": torch.tensor(T_case_meas, dtype=torch.float32),      # (B,T)
            "T_case_true": torch.tensor(T_case_true, dtype=torch.float32),      # (B,T)
            "mask":        torch.tensor(mask, dtype=torch.float32),             # (B,T)
            "P_W":         torch.tensor(P_W, dtype=torch.float32),              # (B,T)
            "T_bplate":    torch.tensor(T_bplate, dtype=torch.float32),         # (B,T)
            "Tamb":        torch.tensor(Tamb, dtype=torch.float32),             # (B,T)
            "dt":          torch.tensor(dt_arr, dtype=torch.float32),           # (B,)
            "scenario_id": torch.tensor(scen_ids, dtype=torch.long),            # (B,)
        }
        return batch_out


# ---------------------------------------------------------------------
# Neural models: MLP, TCN, LSTM (output u_k, not T)
# ---------------------------------------------------------------------
class MLPRegressor(nn.Module):
    """Per-time-step MLP: (B,T,3) -> (B,T) (u_k)."""

    def __init__(self, in_dim: int = 3, hidden: int = 64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, T, D = x.shape
        x_flat = x.reshape(B * T, D)
        u_flat = self.net(x_flat)
        u = u_flat.view(B, T)
        return u


class TCNBlock(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, kernel_size: int = 3, dilation: int = 1, dropout: float = 0.1):
        super().__init__()
        padding = (kernel_size - 1) * dilation
        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)
        self.act = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.resample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,C,T)
        out = self.conv(x)
        T = x.shape[-1]
        out = out[..., -T:]  # cut future padding
        out = self.act(out)
        out = self.dropout(out)
        if self.resample is not None:
            x = self.resample(x)
        return out + x


class TCNRegressor(nn.Module):
    """TCN: (B,T,3) -> (B,T) (u_k)."""

    def __init__(self, in_dim: int = 3, hidden: int = 32, n_blocks: int = 3, kernel_size: int = 3):
        super().__init__()
        layers = []
        in_ch = in_dim
        for i in range(n_blocks):
            out_ch = hidden
            dilation = 2 ** i
            layers.append(
                TCNBlock(
                    in_ch,
                    out_ch,
                    kernel_size=kernel_size,
                    dilation=dilation,
                    dropout=0.1,
                )
            )
            in_ch = out_ch
        self.tcn = nn.Sequential(*layers)
        self.head = nn.Conv1d(in_ch, 1, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,T,3) -> (B,3,T)
        x = x.transpose(1, 2)
        feat = self.tcn(x)
        out = self.head(feat)   # (B,1,T)
        u = out.squeeze(1)      # (B,T)
        return u


class LSTMRegressor(nn.Module):
    """LSTM: (B,T,3) -> (B,T) (u_k)."""

    def __init__(self, in_dim: int = 3, hidden: int = 64, n_layers: int = 1, dropout: float = 0.0):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=in_dim,
            hidden_size=hidden,
            num_layers=n_layers,
            batch_first=True,
            dropout=dropout if n_layers > 1 else 0.0,
        )
        self.head = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out, _ = self.lstm(x)
        u = self.head(out).squeeze(-1)
        return u


# ---------------------------------------------------------------------
# Trainer: ODE integration in the forward pass
# ---------------------------------------------------------------------
class ODEForwardTrainer:
    """
    Trainer for model with ODE integrated in the forward pass:

      T_{k+1} = T_k + dt/C_th * (
        P_k + u_k
        - (T_k - T_bplate_k)/R_cond
        - (T_k - T_amb_k)/R_conv_scenario
      )

    - C_th is learnable (log_Cth)
    - R_cond is a global learnable parameter (log_Rcond)
    - R_conv_scenario is FIXED (from Rconv_list, one per training scenario)
    - u_k is produced by the NN (optional: --no-nn disables it)

    Loss:
      data_loss = MSE(T_pred, T_case_meas) (masked)
      reg_u     = lambda_u * mean(u^2)
      L         = data_loss + reg_u

    This trainer also records training history (losses, R_cond, C_th, reg_u)
    for plotting after training.
    """

    def __init__(
        self,
        model: nn.Module,
        Cth_init: float,
        Rconv_list: List[float],
        device: str = "cpu",
        lambda_u: float = 1e-3,
        clip_grad: float = 5.0,
        use_nn: bool = True,
        Rcond_init: float = 1.0,
    ):
        self.model = model.to(device)
        self.device = torch.device(device)
        self.lambda_u = float(lambda_u)
        self.clip_grad = float(clip_grad)
        self.use_nn = use_nn

        # physical parameters in log-space
        self.log_Rcond = nn.Parameter(
            torch.log(torch.tensor(float(Rcond_init), dtype=torch.float32, device=self.device))
        )
        self.log_Cth = nn.Parameter(
            torch.log(torch.tensor(float(Cth_init), dtype=torch.float32, device=self.device))
        )

        # fixed R_conv per scenario (non-learnable)
        Rconv_arr = torch.tensor(Rconv_list, dtype=torch.float32, device=self.device)
        self.Rconv_all_fixed = Rconv_arr  # shape (n_scen,)

        # parameter list for gradient clipping
        self.params = list(self.model.parameters()) + [self.log_Rcond, self.log_Cth]

        # training history
        self.history = {
            "epoch": [],
            "train_loss": [],
            "val_loss": [],
            "R_cond": [],
            "C_th": [],
            "u_reg": [],
        }

    @property
    def Rcond(self) -> torch.Tensor:
        return torch.exp(self.log_Rcond)  # scalar

    @property
    def Cth(self) -> torch.Tensor:
        return torch.exp(self.log_Cth)    # scalar

    def forward_sequence(self, batch: Dict) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Integrate the ODE over time for the whole batch.

        Returns:
          - T_pred: (B,T)
          - u:      (B,T)
        """
        device = self.device
        inputs = batch["inputs"].to(device)          # (B,T,3)
        P_W = batch["P_W"].to(device)                # (B,T)
        T_bplate = batch["T_bplate"].to(device)      # (B,T)
        Tamb = batch["Tamb"].to(device)              # (B,T)
        dt_seq = batch["dt"].to(device)              # (B,)
        scen_id = batch["scenario_id"].to(device)    # (B,)
        T_meas = batch["T_case_meas"].to(device)     # (B,T)

        B, T = P_W.shape

        # NN: u_k, or u=0 if use_nn=False
        if self.use_nn:
            u = self.model(inputs)                   # (B,T)
        else:
            u = torch.zeros_like(P_W)                # (B,T)

        # initial state: use the first measured value
        T_pred = torch.zeros_like(P_W)
        T_pred[:, 0] = T_meas[:, 0]

        # physical parameters
        Cth = self.Cth                                # scalar
        Rcond = self.Rcond                            # scalar
        Rconv_vec = self.Rconv_all_fixed              # (N_scen,)
        Rconv_eff = Rconv_vec[scen_id].view(B, 1)     # (B,1)

        # dt per batch, assumed constant within each sequence
        if dt_seq.dim() == 0:
            dt_b = dt_seq
        else:
            dt_b = dt_seq.view(B, 1)  # (B,1)

        for k in range(T - 1):
            Tk = T_pred[:, k:k+1]           # (B,1)
            Pk = P_W[:, k:k+1]              # (B,1)
            Tb_k = T_bplate[:, k:k+1]       # (B,1)
            Tamb_k = Tamb[:, k:k+1]         # (B,1)
            uk = u[:, k:k+1]                # (B,1)

            dT = (dt_b / Cth) * (
                Pk + uk
                - (Tk - Tb_k) / Rcond
                - (Tk - Tamb_k) / Rconv_eff
            )
            T_pred[:, k+1:k+2] = Tk + dT

        return T_pred, u

    def step_batch(
        self,
        batch: Dict,
        optimizer: Optional[torch.optim.Optimizer],
        train: bool = True,
    ) -> Tuple[float, float, float]:
        """
        One optimization/eval step on a batch.

        Returns:
          loss_value, data_loss_value, reg_u_value
        """
        device = self.device
        if train:
            self.model.train()
        else:
            self.model.eval()

        T_meas = batch["T_case_meas"].to(device)     # (B,T)
        mask = batch["mask"].to(device)              # (B,T)

        if train and optimizer is not None:
            optimizer.zero_grad()

        T_pred, u = self.forward_sequence(batch)

        # Data loss
        err2 = (T_pred - T_meas) ** 2 * mask
        data_loss = err2.sum() / (mask.sum() + 1e-9)

        # Regularization on u (only if use_nn=True)
        if self.use_nn:
            reg_u = (u ** 2).mean() * self.lambda_u
        else:
            reg_u = torch.tensor(0.0, device=device)

        loss = data_loss + reg_u

        if train and optimizer is not None:
            loss.backward()
            nn.utils.clip_grad_norm_(self.params, max_norm=self.clip_grad)
            optimizer.step()

        return float(loss.item()), float(data_loss.item()), float(reg_u.item())

    def train_epochs(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        epochs: int = 50,
        lr: float = 1e-3,
        lr_phys: float = 1e-2,
    ):
        nn_params = list(self.model.parameters())
        phys_params = [self.log_Rcond, self.log_Cth]

        optimizer = torch.optim.Adam(
            [
                {"params": nn_params,   "lr": lr},
                {"params": phys_params, "lr": lr_phys},
            ]
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="min", factor=0.5, patience=5
        )

        for ep in range(1, epochs + 1):
            train_losses = []
            train_reg_u = []

            for batch in train_loader:
                loss_val, _, reg_u_val = self.step_batch(batch, optimizer, train=True)
                train_losses.append(loss_val)
                train_reg_u.append(reg_u_val)

            train_mean = float(np.mean(train_losses)) if train_losses else float("nan")
            train_u_mean = float(np.mean(train_reg_u)) if train_reg_u else 0.0

            if val_loader is not None:
                val_losses = []
                with torch.no_grad():
                    for batch in val_loader:
                        loss_val, _, _ = self.step_batch(batch, optimizer=None, train=False)
                        val_losses.append(loss_val)
                val_mean = float(np.mean(val_losses)) if val_losses else train_mean
                scheduler.step(val_mean)
            else:
                val_mean = train_mean

            Rcond_val = float(self.Rcond.detach().cpu().numpy())
            Cth_val = float(self.Cth.detach().cpu().numpy())

            # store history
            self.history["epoch"].append(ep)
            self.history["train_loss"].append(train_mean)
            self.history["val_loss"].append(val_mean)
            self.history["R_cond"].append(Rcond_val)
            self.history["C_th"].append(Cth_val)
            self.history["u_reg"].append(train_u_mean)

            if ep == 1 or ep % 10 == 0 or ep == epochs:
                print(
                    f"[Epoch {ep:03d}] train={train_mean:.6f} val={val_mean:.6f} | "
                    f"R_cond={Rcond_val:.4f} K/W, C_th={Cth_val:.4f} J/K, "
                    f"u_reg={train_u_mean:.6e}"
                )

    def export_params(self) -> Dict:
        return {
            "R_cond": float(self.Rcond.detach().cpu().numpy()),
            "C_th": float(self.Cth.detach().cpu().numpy()),
        }


# ---------------------------------------------------------------------
# Utilities: dataloaders, metrics, plots
# ---------------------------------------------------------------------
def make_dataloaders(
    data_dir: Path,
    csv_files: List[str],
    window: int,
    batch_size: int,
    val_fraction: float,
    seed: int,
) -> Tuple[SyntheticThermalWindowDataset, DataLoader, DataLoader]:
    """
    Build windowed dataset and loaders ONLY on training scenarios (csv_files).
    """
    csv_paths = [str(data_dir / name) for name in csv_files]
    if not csv_paths:
        raise SystemExit("No training CSV files specified.")

    ds = SyntheticThermalWindowDataset(
        csv_paths,
        window_size=window,
        stride=max(1, window // 2),
        preload=True,
    )
    ds.compute_normalization()

    rng = np.random.default_rng(seed)
    idx = np.arange(len(ds))
    rng.shuffle(idx)
    n_val = int(len(idx) * val_fraction)
    val_idx = idx[:n_val]
    train_idx = idx[n_val:]

    train_ds = torch.utils.data.Subset(ds, train_idx)
    val_ds = torch.utils.data.Subset(ds, val_idx)

    train_loader = DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=ds.collate_fn,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=ds.collate_fn,
    )
    return ds, train_loader, val_loader


def evaluate_rmse(
    trainer: ODEForwardTrainer,
    loader: DataLoader,
    device: torch.device,
    target_key: str,
) -> float:
    """
    Compute RMSE on T_case_meas or T_case_true using the ODE forward pass
    over windows.

    target_key: "T_case_meas" or "T_case_true".
    """
    assert target_key in ("T_case_meas", "T_case_true")
    sq_err_sum = 0.0
    n_points = 0
    trainer.model.eval()
    with torch.no_grad():
        for batch in loader:
            batch_device = {
                k: v.to(device) if isinstance(v, torch.Tensor) else v
                for k, v in batch.items()
            }
            T_target = batch_device[target_key]
            mask = batch_device["mask"]

            T_pred, _ = trainer.forward_sequence(batch_device)
            err2 = ((T_pred - T_target) ** 2) * mask
            sq_err_sum += float(err2.sum().cpu().numpy())
            n_points += int(mask.sum().cpu().numpy())
    if n_points == 0:
        return float("nan")
    rmse = np.sqrt(sq_err_sum / n_points)
    return float(rmse)


def save_example_plot(
    arch: str,
    trainer: ODEForwardTrainer,
    loader: DataLoader,
    outdir: Path,
    device: torch.device,
):
    """
    Example plot (one validation window):
      - T_case_pred
      - T_case_meas
      - T_case_true
    """
    trainer.model.eval()
    batch = next(iter(loader))
    batch_device = {
        k: v.to(device) if isinstance(v, torch.Tensor) else v
        for k, v in batch.items()
    }

    T_meas = batch_device["T_case_meas"]
    T_true = batch_device["T_case_true"]
    mask = batch_device["mask"]
    dt_seq = batch_device["dt"]

    with torch.no_grad():
        T_pred, _ = trainer.forward_sequence(batch_device)

    T_pred_0 = T_pred[0].cpu().numpy()
    T_meas_0 = T_meas[0].cpu().numpy()
    T_true_0 = T_true[0].cpu().numpy()
    mask_0 = mask[0].cpu().numpy()
    dt0 = float(dt_seq[0].cpu().numpy())
    T_len = len(T_pred_0)
    t_axis = np.arange(T_len) * dt0

    T_meas_plot = np.where(mask_0 > 0.5, T_meas_0, np.nan)
    T_true_plot = np.where(mask_0 > 0.5, T_true_0, np.nan)

    plt.figure(figsize=(10, 4))
    plt.plot(t_axis, T_pred_0, label="T_case pred", linewidth=1.5)
    plt.plot(t_axis, T_meas_plot, label="T_case meas", linestyle="--", linewidth=1.0)
    plt.plot(t_axis, T_true_plot, label="T_case true", linestyle=":", linewidth=1.0)
    plt.xlabel("Time [s]")
    plt.ylabel("Temperature [°C]")
    plt.title(f"{arch.upper()} - Validation example")
    plt.grid(True, alpha=0.3)
    plt.legend()
    out_path = outdir / f"{arch}_val_example.png"
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()
    print(f"[Post] Saved validation example plot for {arch} to {out_path}")


def save_training_curves(
    arch: str,
    trainer: ODEForwardTrainer,
    outdir: Path,
):
    """
    Save training curves for:
      - train/val loss
      - R_cond
      - C_th
      - u_reg (mean lambda_u * mean(u^2) per epoch)
    """
    hist = trainer.history
    epochs = hist["epoch"]
    if not epochs:
        print(f"[Warn] No history to plot for {arch}.")
        return

    train_loss = hist["train_loss"]
    val_loss   = hist["val_loss"]
    R_cond     = hist["R_cond"]
    C_th       = hist["C_th"]
    u_reg      = hist["u_reg"]

    fig, axs = plt.subplots(2, 2, figsize=(10, 8))

    # Losses
    axs[0, 0].plot(epochs, train_loss, label="train")
    axs[0, 0].plot(epochs, val_loss,   label="val")
    axs[0, 0].set_xlabel("Epoch")
    axs[0, 0].set_ylabel("Loss")
    axs[0, 0].set_title("Train / Val loss")
    axs[0, 0].grid(True, alpha=0.3)
    axs[0, 0].legend()

    # R_cond
    axs[0, 1].plot(epochs, R_cond)
    axs[0, 1].set_xlabel("Epoch")
    axs[0, 1].set_ylabel("R_cond [K/W]")
    axs[0, 1].set_title("R_cond evolution")
    axs[0, 1].grid(True, alpha=0.3)

    # C_th
    axs[1, 0].plot(epochs, C_th)
    axs[1, 0].set_xlabel("Epoch")
    axs[1, 0].set_ylabel("C_th [J/K]")
    axs[1, 0].set_title("C_th evolution")
    axs[1, 0].grid(True, alpha=0.3)

    # u_reg
    axs[1, 1].plot(epochs, u_reg)
    axs[1, 1].set_xlabel("Epoch")
    axs[1, 1].set_ylabel("u_reg = lambda_u * mean(u^2)")
    axs[1, 1].set_title("u-regularization term")
    axs[1, 1].grid(True, alpha=0.3)

    plt.suptitle(f"{arch.upper()} - Training curves", y=0.98)
    plt.tight_layout(rect=[0, 0, 1, 0.96])

    out_path = outdir / f"{arch}_training_curves.png"
    plt.savefig(out_path, dpi=150)
    plt.close()
    print(f"[Post] Saved training curves for {arch} to {out_path}")


def evaluate_full_test_scenario(
    arch: str,
    model: nn.Module,
    R_cond_hat: float,
    C_th_hat: float,
    Rconv_test: float,
    csv_path: Path,
    ds_norm: SyntheticThermalWindowDataset,
    device: torch.device,
    lambda_u: float,
    use_nn: bool,
    outdir: Path,
) -> Tuple[float, float]:
    """
    Evaluate the model on a single held-out test scenario, using the FULL
    time sequence (no windows). Use the estimated physical parameters
    (R_cond_hat, C_th_hat) and the true R_conv for that test scenario.

    Returns:
      (rmse_meas_full, rmse_true_full)
    """
    df = pd.read_csv(csv_path)

    for col in ["time_s", "P_W", "Tamb_C", "T_bplate_C", "T_case_true_C", "T_case_meas_C"]:
        if col not in df.columns:
            raise RuntimeError(f"{csv_path}: column '{col}' not found.")

    t = df["time_s"].to_numpy(dtype=float)
    P = df["P_W"].to_numpy(dtype=float)
    Tamb = df["Tamb_C"].to_numpy(dtype=float)
    Tb = df["T_bplate_C"].to_numpy(dtype=float)
    Ttrue = df["T_case_true_C"].to_numpy(dtype=float)
    Tmeas = df["T_case_meas_C"].to_numpy(dtype=float)

    dt = robust_dt(t)
    Tlen = len(t)

    # Build a single batch (B=1, T=Tlen)
    inputs_raw = np.zeros((1, Tlen, 3), dtype=float)
    inputs_raw[0, :, 0] = P
    inputs_raw[0, :, 1] = Tb
    inputs_raw[0, :, 2] = Tamb

    x = inputs_raw.copy()
    for ch in range(3):
        nan_mask = ~np.isfinite(x[:, :, ch])
        x[:, :, ch][nan_mask] = ds_norm.input_mean[ch]

    mean = ds_norm.input_mean.reshape(1, 1, 3)
    std = ds_norm.input_std.reshape(1, 1, 3)
    inputs_norm = (x - mean) / std

    batch = {
        "inputs":      torch.tensor(inputs_norm, dtype=torch.float32, device=device),     # (1,T,3)
        "T_case_meas": torch.tensor(Tmeas.reshape(1, -1), dtype=torch.float32, device=device),
        "T_case_true": torch.tensor(Ttrue.reshape(1, -1), dtype=torch.float32, device=device),
        "mask":        torch.ones((1, Tlen), dtype=torch.float32, device=device),
        "P_W":         torch.tensor(P.reshape(1, -1), dtype=torch.float32, device=device),
        "T_bplate":    torch.tensor(Tb.reshape(1, -1), dtype=torch.float32, device=device),
        "Tamb":        torch.tensor(Tamb.reshape(1, -1), dtype=torch.float32, device=device),
        "dt":          torch.tensor([dt], dtype=torch.float32, device=device),
        "scenario_id": torch.tensor([0], dtype=torch.long, device=device),  # single R_conv
    }

    # Build a "test trainer" wrapper with the estimated physical parameters
    test_trainer = ODEForwardTrainer(
        model=model,
        Cth_init=C_th_hat,
        Rconv_list=[Rconv_test],
        device=str(device),
        lambda_u=lambda_u,
        clip_grad=5.0,
        use_nn=use_nn,
        Rcond_init=R_cond_hat,
    )

    test_trainer.model.eval()
    with torch.no_grad():
        T_pred, _ = test_trainer.forward_sequence(batch)

    T_meas_t = batch["T_case_meas"]
    T_true_t = batch["T_case_true"]
    mask = batch["mask"]

    err2_meas = ((T_pred - T_meas_t) ** 2) * mask
    err2_true = ((T_pred - T_true_t) ** 2) * mask

    n_points = int(mask.sum().cpu().numpy())
    rmse_meas = float(torch.sqrt(err2_meas.sum() / (n_points + 1e-9)).cpu().numpy())
    rmse_true = float(torch.sqrt(err2_true.sum() / (n_points + 1e-9)).cpu().numpy())

    # Full test plot
    T_pred_np = T_pred[0].cpu().numpy()
    T_meas_np = T_meas_t[0].cpu().numpy()
    T_true_np = T_true_t[0].cpu().numpy()
    t_axis = t  # use actual time axis

    plt.figure(figsize=(10, 4))
    plt.plot(t_axis, T_pred_np, label="T_case pred", linewidth=1.5)
    plt.plot(t_axis, T_meas_np, label="T_case meas", linestyle="--", linewidth=1.0)
    plt.plot(t_axis, T_true_np, label="T_case true", linestyle=":", linewidth=1.0)
    plt.xlabel("Time [s]")
    plt.ylabel("Temperature [°C]")
    plt.title(f"{arch.upper()} - FULL test scenario")
    plt.grid(True, alpha=0.3)
    plt.legend()
    out_path = outdir / f"{arch}_test_full.png"
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()
    print(f"[Test] Saved full test plot for {arch} to {out_path}")

    return rmse_meas, rmse_true


# ---------------------------------------------------------------------
# CLI / main
# ---------------------------------------------------------------------
def parse_args():
    ap = argparse.ArgumentParser(
        description=(
            "Training PINN with ODE forward on synthetic thermal dataset "
            "(MLP/TCN/LSTM) - estimating R_cond and C_th with fixed R_conv, "
            "and one held-out test scenario."
        )
    )
    ap.add_argument("--data-dir", required=True, help="Directory with scenario_*.csv and true_params.json")
    ap.add_argument("--outdir", default="results", help="Output directory")
    ap.add_argument("--window", type=int, default=512, help="Window length (samples)")
    ap.add_argument("--batch", type=int, default=8, help="Batch size")
    ap.add_argument("--epochs", type=int, default=80, help="Number of epochs per model")
    ap.add_argument("--lr", type=float, default=1e-3, help="Base learning rate for NN")
    ap.add_argument("--val-frac", type=float, default=0.2, help="Fraction of windows for validation")
    ap.add_argument("--seed", type=int, default=7, help="Random seed")
    ap.add_argument("--lambda-u", type=float, default=1.0, help="Weight for u_k regularization")
    ap.add_argument("--device", default="cpu", help="cpu or cuda:0, etc.")
    ap.add_argument("--arch", choices=["mlp", "tcn", "lstm", "all"], default="all", help="Which architecture to train")
    ap.add_argument("--hidden", type=int, default=8, help="Hidden width for the base models")
    ap.add_argument("--no-nn", action="store_true", help="Disable NN (pure physical parameter fitting)")
    ap.add_argument("--Rcond-init", type=float, default=1.0, help="Initial R_cond [K/W] for optimization")
    ap.add_argument("--Cth-init-scale", type=float, default=0.2, help="Initial scale for C_th: C_th_init = C_th_true * scale")
    ap.add_argument("--Rconv-scale", type=float, default=1.0, help="Global scaling factor applied to R_conv values used by the model")
    ap.add_argument("--test-scen-id", type=int, default=-1, help="scenario_id to use as test (default: last scenario in JSON)")
    return ap.parse_args()


def main():
    args = parse_args()
    data_dir = Path(args.data_dir)
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # Load ground-truth parameters
    json_path = data_dir / "true_params.json"
    if not json_path.exists():
        raise SystemExit(f"{json_path} not found. Run make_synthetic_thermal_dataset.py first.")

    with open(json_path, "r") as f:
        meta = json.load(f)

    Cth_true = float(meta["C_th_true"])
    Rcond_true = float(meta["R_cond_true"])
    scenarios_meta = sorted(meta["scenarios"], key=lambda s: s["scenario_id"])

    # Choose test scenario
    if args.test_scen_id < 0:
        test_scen_id = scenarios_meta[-1]["scenario_id"]  # last one
    else:
        test_scen_id = args.test_scen_id

    test_scen_meta = None
    for s in scenarios_meta:
        if s["scenario_id"] == test_scen_id:
            test_scen_meta = s
            break
    if test_scen_meta is None:
        raise SystemExit(f"Test scenario with id={test_scen_id} not found in true_params.json")

    test_csv_file = test_scen_meta["csv_file"]
    test_csv_path = data_dir / test_csv_file
    Rconv_test_true = float(test_scen_meta["R_conv_true"])

    # Training scenarios: all except test
    train_scenarios = [s for s in scenarios_meta if s["scenario_id"] != test_scen_id]
    train_csv_files = [s["csv_file"] for s in train_scenarios]
    Rconv_train_list = [float(s["R_conv_true"]) for s in train_scenarios]

    # Apply optional scaling to R_conv values actually used by the model.
    # This allows testing robustness to mis-specified convective resistance.
    Rconv_scale = float(args.Rconv_scale)
    Rconv_train_list = [float(s["R_conv_true"]) * Rconv_scale for s in train_scenarios]
    Rconv_test_used = Rconv_test_true * Rconv_scale

    print("=== Ground truth parameters ===")
    print(f"C_th_true   = {Cth_true:.4f} J/K")
    print(f"R_cond_true = {Rcond_true:.4f} K/W")
    print("R_conv_true per scenario:")
    for s in scenarios_meta:
        flag = " (TEST)" if s["scenario_id"] == test_scen_id else ""
        Rconv_true_s = float(s["R_conv_true"])
        Rconv_used_s = Rconv_true_s * Rconv_scale
        print(
            f"  scen {s['scenario_id']:3d}: "
            f"R_conv_true = {Rconv_true_s:.4f} K/W, "
            f"R_conv_used = {Rconv_used_s:.4f} K/W{flag}"
        )

    print(f"\n[R_conv] Global scaling factor: {Rconv_scale:.4f}")
    if Rconv_scale != 1.0:
        print("        (Model intentionally uses mis-scaled R_conv values to probe robustness.)")

    print(f"\n[Split] Held-out TEST scenario: id={test_scen_id}, file={test_csv_file}")
    print(f"[Split] TRAIN scenarios: {[s['scenario_id'] for s in train_scenarios]}")

    print("\n[Main] Building dataloaders (train scenarios only)...")
    ds, train_loader, val_loader = make_dataloaders(
        data_dir=data_dir,
        csv_files=train_csv_files,
        window=args.window,
        batch_size=args.batch,
        val_fraction=args.val_frac,
        seed=args.seed,
    )

    device = torch.device(args.device)

    arch_list = ["mlp", "tcn", "lstm"] if args.arch == "all" else [args.arch]
    results = {}

    Cth_init = Cth_true * float(args.Cth_init_scale)
    Rcond_init = float(args.Rcond_init)

    for arch in arch_list:
        print(f"\n===== Training arch: {arch.upper()} =====")

        # Per-model output directory
        model_outdir = outdir / arch
        model_outdir.mkdir(parents=True, exist_ok=True)

        if arch == "mlp":
            model = MLPRegressor(in_dim=3, hidden=args.hidden)
        elif arch == "tcn":
            model = TCNRegressor(in_dim=3, hidden=args.hidden, n_blocks=3, kernel_size=3)
        elif arch == "lstm":
            model = LSTMRegressor(in_dim=3, hidden=args.hidden, n_layers=1, dropout=0.1)
        else:
            raise ValueError(f"Unknown architecture: {arch}")

        trainer = ODEForwardTrainer(
            model=model,
            Cth_init=Cth_init,
            Rconv_list=Rconv_train_list,
            device=args.device,
            lambda_u=args.lambda_u,
            clip_grad=5.0,
            use_nn=not args.no_nn,
            Rcond_init=Rcond_init,
        )

        # Use a higher LR for physical parameters (e.g. 10x NN LR)
        trainer.train_epochs(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=args.epochs,
            lr=args.lr,
            lr_phys=args.lr * 10.0,
        )

        params = trainer.export_params()
        results[arch] = params
        R_cond_hat = params["R_cond"]
        C_th_hat = params["C_th"]

        # Save checkpoint
        ckpt = {
            "arch": arch,
            "model_state": model.state_dict(),
            "log_Rcond": trainer.log_Rcond.detach().cpu().numpy().tolist(),
            "log_Cth": trainer.log_Cth.detach().cpu().numpy().tolist(),
            "C_th_true": Cth_true,
            "R_cond_true": Rcond_true,
            "R_conv_train_list": Rconv_train_list,
            "test_scen_id": test_scen_id,
            "R_conv_test_true": Rconv_test_true,
            "R_conv_test_used": Rconv_test_used,
            "Rconv_scale": Rconv_scale,
            "dataset_stats": {
                "input_mean": ds.input_mean.tolist(),
                "input_std": ds.input_std.tolist(),
            },
            "history": trainer.history,
        }
        ckpt_path = model_outdir / f"{arch}_final.pt"
        torch.save(ckpt, ckpt_path)
        print(f"[Main] Saved model {arch} checkpoint to {ckpt_path}")

        # Validation RMSE (windows)
        rmse_meas = evaluate_rmse(trainer, val_loader, device, target_key="T_case_meas")
        rmse_true = evaluate_rmse(trainer, val_loader, device, target_key="T_case_true")
        print(f"[Post] {arch.upper()} RMSE (val) vs T_case_meas = {rmse_meas:.4f} °C")
        print(f"[Post] {arch.upper()} RMSE (val) vs T_case_true = {rmse_true:.4f} °C")

        # Example validation plot
        save_example_plot(arch, trainer, val_loader, model_outdir, device)

        # Training curves (loss, R_cond, C_th, u_reg)
        save_training_curves(arch, trainer, model_outdir)

        # --- FULL-SEQUENCE TEST on held-out scenario ---
        rmse_test_meas, rmse_test_true = evaluate_full_test_scenario(
            arch=arch,
            model=model,
            R_cond_hat=R_cond_hat,
            C_th_hat=C_th_hat,
            Rconv_test=Rconv_test_used,
            csv_path=test_csv_path,
            ds_norm=ds,
            device=device,
            lambda_u=args.lambda_u,
            use_nn=not args.no_nn,
            outdir=model_outdir,
        )

        print(f"[Test] {arch.upper()} on test scenario id={test_scen_id}:")
        print(f"       RMSE (FULL) vs T_case_meas = {rmse_test_meas:.4f} °C")
        print(f"       RMSE (FULL) vs T_case_true = {rmse_test_true:.4f} °C")

    # Summary of estimated parameters
    print("\n===== Estimated parameters vs ground truth =====")
    for arch, params in results.items():
        R_cond_hat = params["R_cond"]
        C_th_hat = params["C_th"]
        print(f"\n{arch.upper()}:")
        print(f"  R_cond_true = {Rcond_true:.4f} K/W,  R_cond_hat = {R_cond_hat:.4f} K/W")
        print(f"  C_th_true   = {Cth_true:.4f} J/K,  C_th_hat   = {C_th_hat:.4f} J/K")

    print("\nDone.")


if __name__ == "__main__":
    main()
