#include <stdlib.h>
#include <math.h>
#include "engine.h"
#include "tcn_export.h"   /* generated by export_ipinn_to_c.py */

/* Simple ReLU */
static inline float ipinn_relu(float x) {
    return (x > 0.0f) ? x : 0.0f;
}

/* --------------------------------------------------------------------------
 * Helper: Conv1d as implemented by the TCN blocks in PyTorch
 *
 * PyTorch block used:
 *   nn.Conv1d(in_ch, out_ch, kernel_size, padding=(K-1)*dilation, dilation=dilation)
 *   out = conv(x); T = x.shape[-1]; out = out[..., -T:]
 *
 * This is equivalent to:
 *   y[t] = b + sum_{c_in} sum_{k=0..K-1} w[c_out,c_in,k] * x[c_in, t + k*d]
 * with x[c_in, idx] = 0 if idx < 0 or idx >= T.
 *
 * We implement exactly that.
 * --------------------------------------------------------------------------*/
static void conv1d_tcn_block(
    const float *x,   /* shape: C_in x T, layout: x[c * T + t] */
    int C_in,
    int T,
    const float *w,   /* shape: C_out x C_in x K (flattened) */
    const float *b,   /* shape: C_out */
    int C_out,
    int K,
    int dilation,
    float *y          /* shape: C_out x T */
) {
    for (int co = 0; co < C_out; ++co) {
        const float *b_ptr = &b[co];
        float *y_chan = y + co * T;

        for (int t = 0; t < T; ++t) {
            float acc = *b_ptr;

            for (int ci = 0; ci < C_in; ++ci) {
                const float *x_chan = x + ci * T;
                const float *w_chan = w + ((co * C_in + ci) * K);

                for (int k = 0; k < K; ++k) {
                    int idx = t + k * dilation;
                    if (idx >= 0 && idx < T) {
                        acc += w_chan[k] * x_chan[idx];
                    }
                }
            }
            y_chan[t] = acc;
        }
    }
}

/* --------------------------------------------------------------------------
 * Helper: 1x1 Conv1d (no padding, no dilation, stride=1)
 *
 * Used for:
 *   - resample conv in TCNBlock (1x1)
 *   - final head conv (1x1, C_in -> 1)
 *
 * Formula:
 *   y[t] = b + sum_{c_in} w[c_out,c_in,0] * x[c_in, t]
 * --------------------------------------------------------------------------*/
static void conv1d_pointwise(
    const float *x,   /* shape: C_in x T */
    int C_in,
    int T,
    const float *w,   /* shape: C_out x C_in x 1 */
    const float *b,   /* shape: C_out */
    int C_out,
    float *y          /* shape: C_out x T */
) {
    for (int co = 0; co < C_out; ++co) {
        const float *b_ptr = &b[co];
        float *y_chan = y + co * T;

        for (int t = 0; t < T; ++t) {
            float acc = *b_ptr;
            for (int ci = 0; ci < C_in; ++ci) {
                const float *x_chan = x + ci * T;
                /* weight index: (co * C_in + ci) * 1 + 0 */
                float w_val = w[(co * C_in + ci)];
                acc += w_val * x_chan[t];
            }
            y_chan[t] = acc;
        }
    }
}

/* --------------------------------------------------------------------------
 * Public: get default learned physical parameters
 * --------------------------------------------------------------------------*/
void ipinn_get_default_phys_params(ipinn_phys_params *out) {
    if (!out) return;
    out->R_cond = IPINN_RCOND;
    out->C_th   = IPINN_CTH;
}

/* --------------------------------------------------------------------------
 * TCN forward: compute u_k(t) for a full sequence.
 *
 * Assumes the exported header is for a TCN architecture:
 *   - tcn_tcn_0_conv_weight_DATA / _bias_DATA
 *   - tcn_tcn_1_conv_weight_DATA / _bias_DATA
 *   - tcn_tcn_2_conv_weight_DATA / _bias_DATA
 *   - tcn_tcn_0_resample_weight_DATA / _bias_DATA (only block 0)
 *   - tcn_head_weight_DATA / _bias_DATA
 *
 * Layout is inferred from *_DIMS arrays.
 * --------------------------------------------------------------------------*/
int ipinn_tcn_predict_u(
    const float *P_W,
    const float *T_bplate,
    const float *T_amb,
    int T_len,
    float *u_out
) {
    if (!P_W || !T_bplate || !T_amb || !u_out || T_len <= 0) {
        return -1;
    }

    /* Input dims: expect 3 channels (P, Tb, Tamb) */
    const int C_in0 = 3;

    /* First conv dims: (C_out0, C_in0, K0) */
    const int *dims0 = tcn_tcn_0_conv_weight_DIMS;
    const int C_out0 = dims0[0];
    const int C_in0_check = dims0[1];
    const int K0 = dims0[2];

    if (C_in0_check != C_in0) {
        /* Unexpected dimensions */
        return -2;
    }

    const int *dims1 = tcn_tcn_1_conv_weight_DIMS;
    const int C_out1 = dims1[0];       /* should be hidden */
    const int C_in1  = dims1[1];       /* should be hidden */
    const int K1     = dims1[2];

    const int *dims2 = tcn_tcn_2_conv_weight_DIMS;
    const int C_out2 = dims2[0];       /* should be hidden */
    const int C_in2  = dims2[1];       /* should be hidden */
    const int K2     = dims2[2];

    if (C_out0 != C_out1 || C_out1 != C_out2 || C_in1 != C_out0 || C_in2 != C_out0) {
        /* Hidden dims mismatch */
        return -3;
    }

    const int H = C_out0;  /* hidden channels */

    /* Allocate buffers: input (3 x T), then two hidden buffers and a skip buffer */
    float *x0 = (float *)malloc((size_t)C_in0 * (size_t)T_len * sizeof(float));
    float *buf_a = (float *)malloc((size_t)H * (size_t)T_len * sizeof(float));
    float *buf_b = (float *)malloc((size_t)H * (size_t)T_len * sizeof(float));
    float *buf_skip = (float *)malloc((size_t)H * (size_t)T_len * sizeof(float));

    if (!x0 || !buf_a || !buf_b || !buf_skip) {
        free(x0); free(buf_a); free(buf_b); free(buf_skip);
        return -4;
    }

    /* ----------------------------------------------------------------------
     * 1) Normalize inputs and pack into x0[c * T + t]
     *    x_norm = (x - mean) / std
     * ----------------------------------------------------------------------*/
    const float mean_P   = IPINN_INPUT_MEAN[0];
    const float mean_Tbp = IPINN_INPUT_MEAN[1];
    const float mean_Ta  = IPINN_INPUT_MEAN[2];

    const float std_P    = IPINN_INPUT_STD[0];
    const float std_Tbp  = IPINN_INPUT_STD[1];
    const float std_Ta   = IPINN_INPUT_STD[2];

    float *xP = x0 + 0 * T_len;
    float *xTb = x0 + 1 * T_len;
    float *xTa = x0 + 2 * T_len;

    for (int t = 0; t < T_len; ++t) {
        xP[t]  = (P_W[t]      - mean_P)   / std_P;
        xTb[t] = (T_bplate[t] - mean_Tbp) / std_Tbp;
        xTa[t] = (T_amb[t]    - mean_Ta)  / std_Ta;
    }

    /* ----------------------------------------------------------------------
     * 2) TCN blocks (3 blocks with dilations 1, 2, 4)
     *    Each block:
     *      y = Conv1d(x)
     *      y = ReLU(y)
     *      if resample: x_res = Conv1d_1x1(x) else x_res = x
     *      out = y + x_res
     * ----------------------------------------------------------------------*/

    /* Block 0: in=3, out=H, kernel=K0, dilation=1, resample present (3->H) */
    {
        const float *w0 = tcn_tcn_0_conv_weight_DATA;
        const float *b0 = tcn_tcn_0_conv_bias_DATA;
        const int d0 = 1;

        conv1d_tcn_block(
            x0, C_in0, T_len,
            w0, b0, C_out0, K0, d0,
            buf_a
        );

        /* ReLU */
        for (int i = 0; i < H * T_len; ++i) {
            buf_a[i] = ipinn_relu(buf_a[i]);
        }

        /* Resample skip: 1x1 conv from 3 -> H */
        {
            const float *w_res0 = tcn_tcn_0_resample_weight_DATA;
            const float *b_res0 = tcn_tcn_0_resample_bias_DATA;
            conv1d_pointwise(
                x0, C_in0, T_len,
                w_res0, b_res0, C_out0,
                buf_skip
            );
        }

        /* out = y + x_res */
        for (int i = 0; i < H * T_len; ++i) {
            buf_a[i] += buf_skip[i];
        }
    }

    /* Block 1: in=H, out=H, kernel=K1, dilation=2, identity skip */
    {
        const float *w1 = tcn_tcn_1_conv_weight_DATA;
        const float *b1 = tcn_tcn_1_conv_bias_DATA;
        const int d1 = 2;

        conv1d_tcn_block(
            buf_a, H, T_len,
            w1, b1, H, K1, d1,
            buf_b
        );

        for (int i = 0; i < H * T_len; ++i) {
            buf_b[i] = ipinn_relu(buf_b[i]);
        }

        /* out = y + x (identity skip) */
        for (int i = 0; i < H * T_len; ++i) {
            buf_b[i] += buf_a[i];
        }
    }

    /* Block 2: in=H, out=H, kernel=K2, dilation=4, identity skip */
    {
        const float *w2 = tcn_tcn_2_conv_weight_DATA;
        const float *b2 = tcn_tcn_2_conv_bias_DATA;
        const int d2 = 4;

        conv1d_tcn_block(
            buf_b, H, T_len,
            w2, b2, H, K2, d2,
            buf_a   /* reuse buf_a for output */
        );

        for (int i = 0; i < H * T_len; ++i) {
            buf_a[i] = ipinn_relu(buf_a[i]);
        }

        /* out = y + x (identity skip; x = buf_b) */
        for (int i = 0; i < H * T_len; ++i) {
            buf_a[i] += buf_b[i];
        }
    }

    /* ----------------------------------------------------------------------
     * 3) Head 1x1 Conv1d: H -> 1 (u_k(t)), no activation afterwards
     * ----------------------------------------------------------------------*/
    {
        const int *dims_head = tcn_head_weight_DIMS;
        const int C_out_head = dims_head[0]; /* should be 1 */
        const int C_in_head  = dims_head[1]; /* should be H */
        (void)C_out_head; /* not strictly needed if we assume 1 */
        if (C_in_head != H) {
            free(x0); free(buf_a); free(buf_b); free(buf_skip);
            return -5;
        }

        /* Use buf_b as temporary output of size 1 x T_len */
        conv1d_pointwise(
            buf_a, H, T_len,
            tcn_head_weight_DATA,
            tcn_head_bias_DATA,
            1,
            buf_b
        );

        /* Copy into u_out */
        float *u_chan = buf_b; /* 1 x T_len */
        for (int t = 0; t < T_len; ++t) {
            u_out[t] = u_chan[t];
        }
    }

    free(x0);
    free(buf_a);
    free(buf_b);
    free(buf_skip);
    return 0;
}

/* --------------------------------------------------------------------------
 * Full PINN forward: TCN u_k + ODE integration
 * --------------------------------------------------------------------------*/
int ipinn_tcn_predict_temperature(
    const float *P_W,
    const float *T_bplate,
    const float *T_amb,
    int T_len,
    float dt,
    float R_conv,
    float T_init,
    float *T_pred
) {
    if (!P_W || !T_bplate || !T_amb || !T_pred || T_len <= 0) {
        return -1;
    }

    float *u = (float *)malloc((size_t)T_len * sizeof(float));
    if (!u) {
        return -2;
    }

    int ret = ipinn_tcn_predict_u(P_W, T_bplate, T_amb, T_len, u);
    if (ret != 0) {
        free(u);
        return ret;
    }

    const float R_cond = IPINN_RCOND;
    const float C_th   = IPINN_CTH;

    /* Integrate ODE: T[0] = T_init */
    T_pred[0] = T_init;

    for (int k = 0; k < T_len - 1; ++k) {
        float Tk    = T_pred[k];
        float Pk    = P_W[k];
        float Tb_k  = T_bplate[k];
        float Ta_k  = T_amb[k];
        float uk    = u[k];

        float term_cond = (Tk - Tb_k) / R_cond;
        float term_conv = (Tk - Ta_k) / R_conv;

        float dT = (dt / C_th) * (Pk + uk - term_cond - term_conv);
        T_pred[k + 1] = Tk + dT;
    }

    free(u);
    return 0;
}

/* --------------------------------------------------------------------------
 * Self-test using golden vectors from tcn_export.h
 * --------------------------------------------------------------------------*/
float ipinn_tcn_self_test(void) {
    /* Requires that the export header defined GOLDEN_* and IPINN_RCONV_TEST_USED */
    float *T_pred = (float *)malloc((size_t)GOLDEN_LEN * sizeof(float));
    if (!T_pred) {
        return -1.0f;
    }

    /* Initial condition is the first measured value, as in training */
    float T_init = GOLDEN_T_MEAS[0];

    int ret = ipinn_tcn_predict_temperature(
        GOLDEN_P_W,
        GOLDEN_T_BPLATE,
        GOLDEN_T_AMB,
        GOLDEN_LEN,
        GOLDEN_DT,
        IPINN_RCONV_TEST_USED,
        T_init,
        T_pred
    );

    if (ret != 0) {
        free(T_pred);
        return -2.0f;
    }

    float max_err = 0.0f;
    for (int i = 0; i < GOLDEN_LEN; ++i) {
        float diff = fabsf(T_pred[i] - GOLDEN_T_PRED[i]);
        if (diff > max_err) {
            max_err = diff;
        }
    }

    free(T_pred);
    return max_err;
}
